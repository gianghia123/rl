from typing import Union
from enum import Enum, auto

import numpy as np
import matplotlib.pyplot as plt
import random

from tqdm import trange

from maze import Status, Enviroment

class Agent:
    def __init__(self, seed: Union[str, bytes, int]) -> None:
        self.values = np.full((5 * 5), 1, dtype = np.float64)
        self.policy = np.full((5 * 5, 4), 1, dtype = np.float64)
        # self.policy[99, ::] = [0, 0, 0, 0]
        self.envi = Enviroment(seed = seed)

    def vis_pol(self) -> None:
        self.envi.reset()
        status = Status.IN_PROGRESS
        current_loc = 0
        path = [0]
        while status is Status.IN_PROGRESS:
            directions = {0: 'n', 1: 's', 2: 'w', 3: 'e'}
            d = np.argmax(self.policy[current_loc])
            status, new_loc = self.envi.step(directions[d])
            path.append(new_loc)
        # print(path)
        print(self.envi.trace_path(path))

    def trace(self) -> dict:
        j = self.envi.reset()
        if j == 0:
            pass
        state_reward = {}
        current_loc = 0
        i = 0
        while True:
            # Determine, from the current policy, a move for the current state
            move = random.choices(['n', 's', 'w', 'e'], weights = self.policy[current_loc], k = 1)[0]
            status, reward, new_loc = self.envi.next(move)
            state_reward[(new_loc, i)] = reward
            i += 1
            if status is Status.TERMINATED or status is Status.HALTED:
                break
            current_loc = new_loc
        return state_reward
    
    def approx_values(self) -> None:
        returns = [[] for i in range(25)]
        for i in trange(10000):
            rewards = self.trace()
            visited = set()
            for s, i in rewards:
                if s in visited:
                    continue
                else:
                    visited.add(s)
                    returns[s].append(rewards[(s, i)])
                    self.values[s] = np.mean(returns[s])
            temp = self.values.copy()
            for i in range(25):
                neighbors = np.zeros(4)
                x, y = i % 5, i // 5
                for j, (dx, dy) in enumerate([(0, -1), (0, 1), (-1, 0), (1, 0)]):
                    nx, ny = x + dx, y + dy
                    if (0 <= nx < 5 and 0 <= ny < 5):
                        neighbors[j] = (self.values[ny * 5 + nx])
                    else:
                        pass
                self.policy[i] = np.exp(-neighbors) / np.sum(np.exp(-neighbors))
            # print(temp - self.values)
            if np.all(temp != self.values) and np.linalg.norm(np.abs(temp - self.values)) <= 1e-7:
                break

if __name__ == "__main__":
    agent = Agent(b'imbored')
    print(agent.envi.envi)
    agent.approx_values()
    agent.vis_pol()
    print(agent.policy)
    print(agent.values)
